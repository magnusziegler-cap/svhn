#SVHN

### magnus.c.ziegler@gmail.com

## Challenge
> 1. Write your own solution to the problem "Street View House Numbers" under Format 2 (http://ufldl.stanford.edu/housenumbers/). Make sure to validate your model.
> 2. Write a data loader.  The dataset from step 1 will most likely fit in your RAM memory however, this is rarely the case for larger datasets. We want you to write a data loader that loads the batches "online" meaning that it only reads the data from the disc when it's part of a requested batch. For format 2 you'll get all the data in one file and you can consider indexing this data as reading from the disc.
> The loader should support:
>  - Data shuffling
>  - Batch size
>  - Augmentations - come up with a suitable set of augmentations yourself
>  
> You're not allowed to use pytorch functionality such as Dataset as base and it should > be able to fit in this pseudo code:
>  "for imgs, labels in data_loader.batch_generator():
>      model.train_on_batch(imgs, labels) #This line depends on which framework you use for > training"
> It's fine for you to load the entire dataset into memory during training to speedup the processes but your data loader should be fully integrated in the training code. If you chose to load all the data during training, please submit two solutions. One used for training and the other with your data loader integrated into the code or alternatively some code to visualize the functionality of your data loader.

## Contents
The project folder contains the following scripts and notebooks:
1. svhn_train.ipynb: Notebook showing the workflow for training and validating a classfier for the SVHN problem. For Part 1 of the challenge.
2. svhn_inference.ipynb: Notebook showing the workflow for inferencing the trained model generated in (1.). For Part 1 of the challenge.
3. dataloader.py: code for the DataLoader class used for Part 2 of the challenge.
4. augmentations.py: code for augmentations used for Part 2 of the challenge.
5. svhn_test_dataloader_disk.py: a script to demonstrate the dataloader and some augmentations, where the data is loaded _from disk, on request_.
6. svhn_test_dataloader_memory.py: a script to demonstrate the dataloader and some augmentations, where the data is loaded _from memory (i.e slicing)_
7. writeToDisk.py: a utility script used to read the original .mat files, convert to png, and save to disk. Used to generate some data on-disk for testing (5.).
8. requirements.txt: pip list
9. output.png: basic image depicting training and validation curves, accuracy curve, mcc curve, and confusion matrix
10. readme.md: this file, obviously

The project folder contains the following folders:
1. /models: Trained models were saved here. Best 2 models.
2. /data: original, downloaded-from-source .mat files would live here, but this is blank for keeping .zip size down.
3. /training_data: the .png files generated by 'writeToDisk.py' and used in 'svhn_test_dataloader_disk.py' would live here. They have been
 omitted to keep .zip size down.
